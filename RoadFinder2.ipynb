{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D,UpSampling2D,Lambda, ZeroPadding2D\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "class MyException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOWMANY = 1107\n",
    "MAXLINKS = 1109\n",
    "DEBUG = True\n",
    "TMP_DIR = '/mnt/ramdisk'\n",
    "FORCE_RELOAD = False#True\n",
    "LOAD = True\n",
    "PREPROCESS = True#False\n",
    "batch_size = 128   # ile obrazków przetwarzamy na raz (aktualizacja wag sieci następuje raz na całą grupę obrazków)\n",
    "epochs = 12         # ile epok będziemy uczyli\n",
    "SIZE = (750,750)\n",
    "SIDE = 250\n",
    "IMPOSITION = 15\n",
    "HOWMANYPERIMAGE = int(SIZE[0]*SIZE[1]/SIDE/SIDE)\n",
    "IMAGESPERFILE = 9\n",
    "assert int(SIZE[0]*SIZE[1]/SIDE/SIDE)==HOWMANYPERIMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(url):\n",
    "    raw = urllib.request.urlopen(url).read()\n",
    "    npraw= np.array(bytearray(raw),dtype=np.uint8)\n",
    "    return cv2.imdecode(npraw,-1)#-1 -> as is (with the alpha channel)\n",
    "\n",
    "def getImageName(url):\n",
    "    return url.split('/').pop().split('.').pop(0)\n",
    "\n",
    "def pickleBigDataset(prefix,dataset,size):\n",
    "    j = int(np.ceil(len(dataset)/size))\n",
    "    for i in range(1,j+1):\n",
    "        np.save(os.path.join(TMP_DIR, prefix+str(i)),np.array(dataset[size*(i-1):size*i]))\n",
    "\n",
    "def unpickleBigDataset(prefix):\n",
    "    onlyfiles = [f for f in os.listdir(TMP_DIR) if os.path.isfile(os.path.join(TMP_DIR, f))\n",
    "                 and f.startswith(prefix)]\n",
    "    dataset = []\n",
    "    if len(onlyfiles)>0:\n",
    "        print(\"Loading...\")\n",
    "        dataset = np.load(os.path.join(TMP_DIR, onlyfiles[0]))\n",
    "        print(\"Loaded first\")\n",
    "        for f in tqdm_notebook(onlyfiles[1:]):\n",
    "            dataset=np.append(dataset,np.load(os.path.join(TMP_DIR, f)),axis=0)\n",
    "    return dataset\n",
    "#     return np.load(os.path.join(TMP_DIR, \"{}.npy\".format(prefix)))\n",
    "    \n",
    "            \n",
    "def loadImagesFromSite(url,prefix):\n",
    "    onlyfiles = [f for f in os.listdir(TMP_DIR) if os.path.isfile(os.path.join(TMP_DIR, f)) and f.startswith(prefix)]\n",
    "    if len(onlyfiles)==0 or FORCE_RELOAD:\n",
    "        imgs = []\n",
    "        I = None\n",
    "        \n",
    "    else:\n",
    "        imgs = [img for img in unpickleBigDataset(prefix)[:HOWMANY]]\n",
    "        I = len(imgs)\n",
    "    print(\"Cached images {}.\".format(I if I is not None else 0))\n",
    "    \n",
    "    if (HOWMANY is not None and len(imgs)<HOWMANY and len(imgs)<MAXLINKS) or (len(imgs)<MAXLINKS and HOWMANY is None):\n",
    "        print(\"Loading images from {}\".format(url))\n",
    "        print(\"Proceeding from {} image.\".format(I if I is not None else 0))\n",
    "\n",
    "        s = IMAGESPERFILE\n",
    "\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html = BeautifulSoup(response.read(),\"lxml\")\n",
    "            i = I if I is not None else 0\n",
    "            links = html.find_all('a')[I:HOWMANY]\n",
    "            for link in tqdm_notebook(links):\n",
    "                img = loadImage(link.get('href'))  \n",
    "                img = cv2.resize(img,SIZE)\n",
    "#                 print(link.get('href'))\n",
    "                imgs += [cv2.resize(img,SIZE)]\n",
    "                if i%s==0:\n",
    "                    pickleBigDataset(prefix,imgs,s)\n",
    "                i+=1\n",
    "        pickleBigDataset(prefix,imgs,s)\n",
    "    \n",
    "        \n",
    "    return np.array(imgs)  \n",
    "\n",
    "def saveDataset(X,Y,prefix=\"\"):\n",
    "    with open('pickledDatasetX'+prefix,'wb') as f:\n",
    "        pickle.dump(X,f)\n",
    "    with open('pickledDatasetY'+prefix,'wb') as f:\n",
    "        pickle.dump(Y,f)\n",
    "        \n",
    "def loadDataset(prefix=\"\"):\n",
    "    try:\n",
    "        X = unpickleBigDataset('x')\n",
    "        Y = unpickleBigDataset('y')\n",
    "        if len(X) == len(Y) and len(X) == HOWMANY:\n",
    "            return X,Y\n",
    "        else:\n",
    "            print(\"Failed loading dataset from file system\")\n",
    "            return None,None\n",
    "    except:\n",
    "        print(\"Failed loading dataset from file system\")\n",
    "        return None,None\n",
    "    \n",
    "def display(X,Y,howmany=None):\n",
    "    if howmany is None:\n",
    "        howmany = X.shape[0]\n",
    "        \n",
    "    for i in range(howmany):\n",
    "        print(X[i].max(),X[i].min())\n",
    "        plt.figure()\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(X[i])\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(Y[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_patches(image,size,side,imposition):\n",
    "#     patches = []\n",
    "    \n",
    "    \n",
    "#     for i in range(int(size[0]/side)):\n",
    "#         for j in range(int(size[1]/side)):\n",
    "#             patches += [image[i*side:(i+1)*side,j*side:(j+1)*side]]\n",
    "#     return patches\n",
    "\n",
    "def get_patches(image,size,side,imposition):\n",
    "    patches = []\n",
    "    \n",
    "    if len(image.shape)==3:\n",
    "        img = np.zeros((image.shape[0]+imposition,image.shape[1]+imposition,3))\n",
    "        for i in range(3):\n",
    "            img[...,i] = np.pad(image[...,i],((imposition,0),(imposition,0)),'reflect')\n",
    "        image = img\n",
    "    else:\n",
    "        image = np.pad(image,((imposition,0),(imposition,0)),'reflect')\n",
    "\n",
    "    for i in range(int(size[0]/side)):\n",
    "        for j in range(int(size[1]/side)):\n",
    "            imp1=np.max([i*side-imposition,0])\n",
    "            imp2=(i+1)*side+imposition if imp1!=0 else (i+1)*side+imposition*2\n",
    "            imp3=np.max([j*side-imposition,0])\n",
    "            imp4=(j+1)*side+imposition if imp3!=0 else (j+1)*side+imposition*2\n",
    "            patches += [image[imp1:imp2,imp3:imp4]]\n",
    "    return patches\n",
    "\n",
    "def preprocessorX(image):\n",
    "    size,side,imposition = SIZE,SIDE,IMPOSITION\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    if image.max() > 1:\n",
    "        image /= 255\n",
    "\n",
    "    img = np.zeros((image.shape[0], image.shape[1], 4))\n",
    "    img[...,4] = max(image,axis=2) - min(image,axis=2)\n",
    "    img[...,4] -= image[...,4].mean()\n",
    "    img[...,4] /= image[...,4].std()\n",
    "    for i in range(3):\n",
    "        image[...,i] -= image[...,i].mean()\n",
    "        image[...,i] /= image[...,i].std()\n",
    "        img[...,i] = image[...,i]\n",
    "    \n",
    "\n",
    "    #remove outliers\n",
    "    image[image<-3] = -3\n",
    "    image[image>3] = 3\n",
    "\n",
    "    #between -1,1\n",
    "    for i in range(4):\n",
    "        image[...,i] /= np.max(np.abs([image[...,i].min(),image[...,i].max()]))\n",
    "\n",
    "    return get_patches(image,size,side,imposition)\n",
    "    \n",
    "def preprocessorY(image):\n",
    "    size,side,imposition = SIZE,SIDE,IMPOSITION\n",
    "\n",
    "    image = image.astype(np.float32)\n",
    "    if image.max() > 1:\n",
    "        image /= 255\n",
    "    for i in range(3):\n",
    "        image[...,i] = (image[...,i] - image[...,i].min())/(image[...,i].max() - image[...,i].min())\n",
    "    return get_patches(image,size,side,imposition)\n",
    "    \n",
    "def getRoadStats(arr,mask):\n",
    "    b = mask.astype(np.bool)\n",
    "    x = arr[b]\n",
    "    if len(x) != 0:\n",
    "        return [x.max(0),x.min(0),x.mean(0),x.std(0),np.median(x,axis=0)]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def preprocessXY(X,Y):\n",
    "    \n",
    "    r = []\n",
    "    for i in range(len(X)):\n",
    "        s = getRoadStats(X[i],Y[i])\n",
    "        if s is not None:\n",
    "            r += [s]\n",
    "            \n",
    "    return np.array(r).mean(0)\n",
    "    \n",
    "\n",
    "def preprocess(images,preprocessor,prefix):\n",
    "    onlyfiles = [f for f in os.listdir(TMP_DIR) if os.path.isfile(os.path.join(TMP_DIR, f)) and f.startswith(prefix)]\n",
    "    if len(onlyfiles)==0:\n",
    "        I = None\n",
    "        result = []\n",
    "    else:\n",
    "        result = unpickleBigDataset(prefix)[:HOWMANY*HOWMANYPERIMAGE]\n",
    "        I = int(len(result)/HOWMANYPERIMAGE)\n",
    "    print(\"Cached images {}.\".format(len(result)))\n",
    "    \n",
    "    s = int(IMAGESPERFILE*HOWMANYPERIMAGE / 4 )\n",
    "    if len(result)<HOWMANY*HOWMANYPERIMAGE:\n",
    "        print(\"Preprocessing images.\")\n",
    "        print(\"Proceeding from {} image.\".format(I if I is not None else 0))\n",
    "        i = I if I is not None else 0\n",
    "        ignoring = []\n",
    "        for image in tqdm_notebook(images[I:]):\n",
    "            r = preprocessor(image)\n",
    "            result += r\n",
    "#             i += HOWMANYPERIMAGE\n",
    "#             if i%s==0:\n",
    "#                 pickleBigDataset(prefix,result,s)\n",
    "        images = None\n",
    "        gc.collect()\n",
    "        pickleBigDataset(prefix,result,s)\n",
    "    images = None\n",
    "        \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doSomeDeepLearning(X=None,Y=None,side=280):\n",
    "    num_classes = 2    # ile klas będziemy rozpoznawali\n",
    "\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = side,side   # takie wymiary mają obrazki w bazie MNIST\n",
    "\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    try:\n",
    "        x_train = unpickleBigDataset('xain')\n",
    "        y_train = unpickleBigDataset('yain')\n",
    "        x_test = unpickleBigDataset('xest')\n",
    "        y_test = unpickleBigDataset('yest')\n",
    "        if len(x_train)==0 or len(y_train)==0 or len(x_test)==0 or len(y_test)==0:\n",
    "            raise Exception\n",
    "        if len(x_train) + len(x_test)!=HOWMANY:\n",
    "            raise MyException\n",
    "    except:\n",
    "        if X is None or Y is None:\n",
    "            raise MyException\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "            x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "            y_train = y_train.reshape(y_train.shape[0], 1, img_rows, img_cols)\n",
    "            y_test = y_test.reshape(y_test.shape[0], 1, img_rows, img_cols)\n",
    "            input_shape = (1, img_rows, img_cols)\n",
    "        else:\n",
    "            x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
    "            x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
    "            y_train = y_train.reshape(y_train.shape[0], img_rows, img_cols, 1)\n",
    "            y_test = y_test.reshape(y_test.shape[0], img_rows, img_cols, 1)\n",
    "            input_shape = (img_rows, img_cols, 1)\n",
    "        s = IMAGESPERFILE * HOWMANYPERIMAGE\n",
    "        pickleBigDataset('xain',x_train,s)\n",
    "        pickleBigDataset('yain',y_train,s)\n",
    "        pickleBigDataset('xest',x_test,s)\n",
    "        pickleBigDataset('yest',y_test,s)\n",
    "\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    \n",
    "    curr_epoch = -1\n",
    "    onlyfiles = [f for f in os.listdir('.') if os.path.isfile(os.path.join('.', f)) and f.startswith('moj_ulubiony_model') and f.endswith('.h5')]\n",
    "    if len(onlyfiles) == 0:\n",
    "        print(\"No saved model. Preparing model.\")\n",
    "        imput = Input(shape=(side,side,4))\n",
    "        conv1 = Conv2D(32, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(imput)\n",
    "        conv1 = Conv2D(32,\n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv1)\n",
    "        conv1 = Conv2D(32, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv1)\n",
    "        dropout1 = Dropout(0.2)(conv1)\n",
    "        maxpool1 = MaxPooling2D(pool_size=(2, 2))(dropout1)\n",
    "        conv2 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(maxpool1)\n",
    "        conv2 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv2)\n",
    "        conv2 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv2)\n",
    "        dropout2 = Dropout(0.25)(conv2)\n",
    "        maxpool2 = MaxPooling2D(pool_size=(2, 2))(dropout2)\n",
    "        conv3 = Conv2D(128, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(maxpool2)\n",
    "        conv3 = Conv2D(128, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv3)\n",
    "        conv3 = Conv2D(128, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv3)\n",
    "        dropout3 = Dropout(0.25)(conv3)\n",
    "        upsample1 = UpSampling2D(size=(2,2))(dropout3)\n",
    "        \n",
    "        concat1 = concatenate([upsample1,conv2,])#lambda1])\n",
    "        conv4 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(concat1)\n",
    "        conv4 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv4)\n",
    "        conv4 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv4)\n",
    "        dropout4 = Dropout(0.25)(conv4)\n",
    "        upsample2 = UpSampling2D(size=(2,2))(dropout4)\n",
    "        zpad1 = ZeroPadding2D(((1,0),(1,0)))(upsample2)\n",
    "        concat2 = concatenate([zpad1,conv1])\n",
    "        conv5 = Conv2D(32, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(concat2)\n",
    "        conv5 = Conv2D(32, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv5)\n",
    "        conv5 = Conv2D(1, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv5)\n",
    "        model = Model(inputs=imput, outputs=conv5)\n",
    "\n",
    "        model.compile(loss=keras.losses.mean_squared_error,\n",
    "                  optimizer=keras.optimizers.Adam(),       \n",
    "                  metrics=['accuracy']) \n",
    "    elif len(onlyfiles) == 1:\n",
    "        print(\"Saved model:\\\"{}\\\"\".format(onlyfiles[0]))\n",
    "        model = keras.models.load_model(onlyfiles[0])\n",
    "    else:\n",
    "        onlyfiles = map(lambda y:filter(lambda x:x is not None and x.startswith('epoch'),y.split('.')[0].split('_')),onlyfiles)\n",
    "        curr_epoch = max(list(map(lambda x:int(x[4:]),onlyfiles)))\n",
    "        model = keras.models.load_model(\"moj_ulubiony_model_epoch{}.h5\".format(curr_epoch))\n",
    "        print(\"Saved model:\\\"moj_ulubiony_model_epoch{}.h5\\\"\".format(curr_epoch))\n",
    "    curr_epoch += 1\n",
    "    print(\"Current epoch:{}\".format(curr_epoch))\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    for i in range(curr_epoch,epochs):\n",
    "        model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=1,\n",
    "                      verbose=1,\n",
    "                      validation_data=(x_test, y_test))\n",
    "        model.save(\"moj_ulubiony_model_epoch{}.h5\".format(i))\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images (X)\n",
      "Cached images 0.\n",
      "Loading images from https://www.cs.toronto.edu/~vmnih/data/mass_roads/train/sat/index.html\n",
      "Proceeding from 0 image.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ba179099e4479eb3092d862202f94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1107), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-57433f332572>\u001b[0m in \u001b[0;36mdoSomeDeepLearning\u001b[0;34m(X, Y, side)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mHOWMANY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMyException\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1bc827312ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mdoSomeDeepLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mMyException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-57433f332572>\u001b[0m in \u001b[0;36mdoSomeDeepLearning\u001b[0;34m(X, Y, side)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMyException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMyException\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1bc827312ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mLOAD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading images (X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadImagesFromSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'f'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPREPROCESS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocessing images (X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d4dabaad5727>\u001b[0m in \u001b[0;36mloadImagesFromSite\u001b[0;34m(url, prefix)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mHOWMANY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#                 print(link.get('href'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d4dabaad5727>\u001b[0m in \u001b[0;36mloadImage\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnpraw\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpraw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#-1 -> as is (with the alpha channel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    try:\n",
    "        doSomeDeepLearning()\n",
    "    except MyException as e:\n",
    "        urlX = \"https://www.cs.toronto.edu/~vmnih/data/mass_roads/train/sat/index.html\"\n",
    "        urlY = \"https://www.cs.toronto.edu/~vmnih/data/mass_roads/train/map/index.html\"\n",
    "\n",
    "        if LOAD:\n",
    "            print(\"Loading images (X)\")\n",
    "            X = loadImagesFromSite(urlX,'f')\n",
    "        if PREPROCESS:\n",
    "            print(\"Preprocessing images (X)\")\n",
    "            X = preprocess(X,preprocessorX,'x')\n",
    "        if LOAD:\n",
    "            print(\"Loading images (Y)\")\n",
    "            Y = loadImagesFromSite(urlY,'z')\n",
    "        if PREPROCESS:\n",
    "            print(\"Preprocessing images (Y)\")\n",
    "            Y = preprocess(Y,preprocessorY,'y')\n",
    "\n",
    "            r = preprocessXY(X,Y)\n",
    "            print(\"\\n\\t| {}\\t\\t\\t| {}\\t\\t| {}\".format('Hue', 'Saturation', 'Value'))\n",
    "            l = ['max','min','avg','std','median']\n",
    "            for i,(c1, c2, c3) in enumerate(r):  \n",
    "                print(\"{}\\t| {}\\t| {}\\t| {}\".format(l[i],c1, c2, c3))\n",
    "\n",
    "        doSomeDeepLearning(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [2, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.array([[[1,2,3], [1,2,4]],[[4,2,3], [3,2,2]]])\n",
    "np.max(x,axis=2)-np.min(x,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
