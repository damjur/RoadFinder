{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import os\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, concatenate, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D,UpSampling2D,Lambda, Activation\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import gc\n",
    "import collections\n",
    "\n",
    "class MyException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOWMANY = 1107\n",
    "MAXLINKS = 1109\n",
    "DEBUG = True\n",
    "TMP_DIR = 'C:\\\\Tmp'\n",
    "FORCE_RELOAD = False#True\n",
    "LOAD = True\n",
    "PREPROCESS = True#False\n",
    "batch_size = 128   # ile obrazków przetwarzamy na raz (aktualizacja wag sieci następuje raz na całą grupę obrazków)\n",
    "epochs = 12         # ile epok będziemy uczyli\n",
    "SIZE = (750,750)\n",
    "SIDE = 250\n",
    "IMPOSITION = 15\n",
    "HOWMANYPERIMAGE = int(SIZE[0]*SIZE[1]/SIDE/SIDE)\n",
    "IMAGESPERFILE = 9\n",
    "assert int(SIZE[0]*SIZE[1]/SIDE/SIDE)==HOWMANYPERIMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(url):\n",
    "    raw = urllib.request.urlopen(url).read()\n",
    "    npraw= np.array(bytearray(raw),dtype=np.uint8)\n",
    "    return cv2.imdecode(npraw,-1)#-1 -> as is (with the alpha channel)\n",
    "\n",
    "def getImageName(url):\n",
    "    return url.split('/').pop().split('.').pop(0)\n",
    "\n",
    "def pickleBigDataset(prefix,dataset,size):\n",
    "    j = int(np.ceil(len(dataset)/size))\n",
    "    for i in range(1,j+1):\n",
    "        np.save(os.path.join(TMP_DIR, prefix+str(i)),np.array(dataset[size*(i-1):size*i]))\n",
    "\n",
    "def unpickleBigDataset(prefix):\n",
    "    onlyfiles = [f for f in os.listdir(TMP_DIR) if os.path.isfile(os.path.join(TMP_DIR, f))\n",
    "                 and f.startswith(prefix)]\n",
    "    dataset = []\n",
    "    if len(onlyfiles)>0:\n",
    "        print(\"Loading...\")\n",
    "        dataset = np.load(os.path.join(TMP_DIR, onlyfiles[0]))\n",
    "        print(\"Loaded first\")\n",
    "        for f in tqdm_notebook(onlyfiles[1:]):\n",
    "            dataset=np.append(dataset,np.load(os.path.join(TMP_DIR, f)),axis=0)\n",
    "    return dataset\n",
    "\n",
    "def unpickleBigDataset2(prefix,start,stop):\n",
    "    onlyfiles = [f for f in os.listdir(TMP_DIR) if os.path.isfile(os.path.join(TMP_DIR, f))\n",
    "                 and f.startswith(prefix)]\n",
    "    dataset = []\n",
    "    if len(onlyfiles)>0:\n",
    "        print(\"Loading...\")\n",
    "        dataset = np.load(os.path.join(TMP_DIR, prefix+str(start)+\".npy\"))\n",
    "        print(\"Loaded first\")\n",
    "        for f in range(start+1,stop+1):\n",
    "            dataset=np.append(dataset,np.load(os.path.join(TMP_DIR, prefix+str(f)+\".npy\")),axis=0)\n",
    "    return dataset\n",
    "#     return np.load(os.path.join(TMP_DIR, \"{}.npy\".format(prefix)))\n",
    "    \n",
    "            \n",
    "def loadImagesFromSite(url,prefix):\n",
    "    onlyfiles = [f for f in os.listdir(TMP_DIR) if os.path.isfile(os.path.join(TMP_DIR, f)) and f.startswith(prefix)]\n",
    "    if len(onlyfiles)==0 or FORCE_RELOAD:\n",
    "        imgs = []\n",
    "        I = None\n",
    "        \n",
    "    else:\n",
    "        imgs = [img for img in unpickleBigDataset(prefix)[:HOWMANY]]\n",
    "        I = len(imgs)\n",
    "    print(\"Cached images {}.\".format(I if I is not None else 0))\n",
    "    \n",
    "    if (HOWMANY is not None and len(imgs)<HOWMANY and len(imgs)<MAXLINKS) or (len(imgs)<MAXLINKS and HOWMANY is None):\n",
    "        print(\"Loading images from {}\".format(url))\n",
    "        print(\"Proceeding from {} image.\".format(I if I is not None else 0))\n",
    "\n",
    "        s = IMAGESPERFILE\n",
    "\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html = BeautifulSoup(response.read(),\"lxml\")\n",
    "            i = I if I is not None else 0\n",
    "            links = html.find_all('a')[I:HOWMANY]\n",
    "            for link in tqdm_notebook(links):\n",
    "                img = loadImage(link.get('href'))  \n",
    "                img = cv2.resize(img,SIZE)\n",
    "#                 print(link.get('href'))\n",
    "                imgs += [cv2.resize(img,SIZE)]\n",
    "                if i%s==0:\n",
    "                    pickleBigDataset(prefix,imgs,s)\n",
    "                i+=1\n",
    "        pickleBigDataset(prefix,imgs,s)\n",
    "    \n",
    "        \n",
    "    return np.array(imgs)  \n",
    "\n",
    "def saveDataset(X,Y,prefix=\"\"):\n",
    "    with open('pickledDatasetX'+prefix,'wb') as f:\n",
    "        pickle.dump(X,f)\n",
    "    with open('pickledDatasetY'+prefix,'wb') as f:\n",
    "        pickle.dump(Y,f)\n",
    "        \n",
    "def loadDataset(prefix=\"\"):\n",
    "    try:\n",
    "        X = unpickleBigDataset('x')\n",
    "        Y = unpickleBigDataset('y')\n",
    "        if len(X) == len(Y) and len(X) == HOWMANY:\n",
    "            return X,Y\n",
    "        else:\n",
    "            print(\"Failed loading dataset from file system\")\n",
    "            return None,None\n",
    "    except:\n",
    "        print(\"Failed loading dataset from file system\")\n",
    "        return None,None\n",
    "    \n",
    "def display(X,Y,howmany=None):\n",
    "    if howmany is None:\n",
    "        howmany = X.shape[0]\n",
    "        \n",
    "    for i in range(howmany):\n",
    "        print(X[i].max(),X[i].min())\n",
    "        plt.figure()\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(X[i])\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(Y[i])\n",
    "        \n",
    "def create_weighted_binary_crossentropy(zero_weight, one_weight):\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "\n",
    "        # Original binary crossentropy (see losses.py):\n",
    "        # K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "        # Calculate the binary crossentropy\n",
    "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # Apply the weights\n",
    "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "        # Return the mean error\n",
    "        return K.mean(weighted_b_ce)\n",
    "\n",
    "    return weighted_binary_crossentropy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_patches(image,size,side,imposition):\n",
    "#     patches = []\n",
    "    \n",
    "    \n",
    "#     for i in range(int(size[0]/side)):\n",
    "#         for j in range(int(size[1]/side)):\n",
    "#             patches += [image[i*side:(i+1)*side,j*side:(j+1)*side]]\n",
    "#     return patches\n",
    "\n",
    "def get_patches(image,size,side,imposition):\n",
    "    patches = []\n",
    "    \n",
    "    if len(image.shape)==3:\n",
    "        img = np.zeros((image.shape[0]+imposition,image.shape[1]+imposition,4))\n",
    "        for i in range(4):\n",
    "            img[...,i] = np.pad(image[...,i],((imposition,0),(imposition,0)),'reflect')\n",
    "        image = img\n",
    "    else:\n",
    "        image = np.pad(image,((imposition,0),(imposition,0)),'reflect')\n",
    "\n",
    "    for i in range(int(size[0]/side)):\n",
    "        for j in range(int(size[1]/side)):\n",
    "            imp1=np.max([i*side-imposition,0])\n",
    "            imp2=(i+1)*side+imposition if imp1!=0 else (i+1)*side+imposition*2\n",
    "            imp3=np.max([j*side-imposition,0])\n",
    "            imp4=(j+1)*side+imposition if imp3!=0 else (j+1)*side+imposition*2\n",
    "            patches += [image[imp1:imp2,imp3:imp4]]\n",
    "    return patches\n",
    "\n",
    "def preprocessorX(image):\n",
    "    size,side,imposition = SIZE,SIDE,IMPOSITION\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    if image.max() > 1:\n",
    "        image /= 255\n",
    "\n",
    "    img = np.zeros((image.shape[0], image.shape[1], 4))\n",
    "    img[...,3] = np.max(image,axis=2) - np.min(image,axis=2)\n",
    "    img[...,3] -= img[...,3].mean()\n",
    "    img[...,3] /= img[...,3].std()\n",
    "    for i in range(3):\n",
    "        image[...,i] -= image[...,i].mean()\n",
    "        image[...,i] /= image[...,i].std()\n",
    "        img[...,i] = image[...,i]\n",
    "    \n",
    "\n",
    "    #remove outliers\n",
    "    img[img<-3] = -3\n",
    "    img[img>3] = 3\n",
    "\n",
    "    #between -1,1\n",
    "    for i in range(4):\n",
    "        img[...,i] /= np.max(np.abs([img[...,i].min(),img[...,i].max()]))\n",
    "\n",
    "    return get_patches(img,size,side,imposition)\n",
    "    \n",
    "def preprocessorY(image):\n",
    "    size,side,imposition = SIZE,SIDE,IMPOSITION\n",
    "\n",
    "    image = image.astype(np.float32)\n",
    "    image[image != 0] = 1\n",
    "    return get_patches(image,size,side,imposition)\n",
    "    \n",
    "def getRoadStats(arr,mask):\n",
    "    b = mask.astype(np.bool)\n",
    "    x = arr[b]\n",
    "    if len(x) != 0:\n",
    "        return [x.max(0),x.min(0),x.mean(0),x.std(0),np.median(x,axis=0)]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def preprocessXY(X,Y):\n",
    "    \n",
    "    r = []\n",
    "    for i in range(len(X)):\n",
    "        s = getRoadStats(X[i],Y[i])\n",
    "        if s is not None:\n",
    "            r += [s]\n",
    "            \n",
    "    return np.array(r).mean(0)\n",
    "    \n",
    "\n",
    "def preprocess(images,preprocessor,prefix):\n",
    "    onlyfiles = [f for f in os.listdir(TMP_DIR) if os.path.isfile(os.path.join(TMP_DIR, f)) and f.startswith(prefix)]\n",
    "    if len(onlyfiles)==0:\n",
    "        I = None\n",
    "        result = []\n",
    "    else:\n",
    "        result = unpickleBigDataset(prefix)[:HOWMANY*HOWMANYPERIMAGE]\n",
    "        I = int(len(result)/HOWMANYPERIMAGE)\n",
    "    print(\"Cached images {}.\".format(len(result)))\n",
    "    \n",
    "    s = int(IMAGESPERFILE*HOWMANYPERIMAGE / 4 )\n",
    "    if len(result)<HOWMANY*HOWMANYPERIMAGE:\n",
    "        print(\"Preprocessing images.\")\n",
    "        print(\"Proceeding from {} image.\".format(I if I is not None else 0))\n",
    "        i = I if I is not None else 0\n",
    "        ignoring = []\n",
    "        for image in tqdm_notebook(images[I:]):\n",
    "            r = preprocessor(image)\n",
    "            result += r\n",
    "#             i += HOWMANYPERIMAGE\n",
    "#             if i%s==0:\n",
    "#                 pickleBigDataset(prefix,result,s)\n",
    "        images = None\n",
    "        gc.collect()\n",
    "        pickleBigDataset(prefix,result,s)\n",
    "    images = None\n",
    "        \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doSomeDeepLearning(X=None,Y=None,side=280):\n",
    "    num_classes = 2    # ile klas będziemy rozpoznawali\n",
    "\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = side,side   # takie wymiary mają obrazki w bazie MNIST\n",
    "\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    try:\n",
    "        x_train = unpickleBigDataset('xain')\n",
    "        y_train = unpickleBigDataset('yain')\n",
    "        x_test = unpickleBigDataset('xest')\n",
    "        y_test = unpickleBigDataset('yest')\n",
    "        if len(x_train)==0 or len(y_train)==0 or len(x_test)==0 or len(y_test)==0:\n",
    "            raise Exception\n",
    "        if len(x_train) + len(x_test)!=HOWMANY:\n",
    "            raise MyException\n",
    "    except:\n",
    "        x_test = unpickleBigDataset2(\"x\",461,499)\n",
    "        y_test = unpickleBigDataset2(\"y\",461,499)\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            x_test = x_test.reshape(x_test.shape[0], 4, img_rows, img_cols)\n",
    "            y_test = y_test.reshape(y_test.shape[0], img_rows * img_cols)\n",
    "            input_shape = (4, img_rows, img_cols)\n",
    "        else:\n",
    "            x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 4)\n",
    "            y_test = y_test.reshape(y_test.shape[0], img_rows * img_cols)\n",
    "            input_shape = (img_rows, img_cols, 4)\n",
    "        s = IMAGESPERFILE * HOWMANYPERIMAGE\n",
    "#         pickleBigDataset('xain',x_train,s)\n",
    "#         pickleBigDataset('yain',y_train,s)\n",
    "#         pickleBigDataset('xest',x_test,s)\n",
    "#         pickleBigDataset('yest',y_test,s)\n",
    "\n",
    "#     print('x_train shape:', x_train.shape)\n",
    "#     print(x_train.shape[0], 'train samples')\n",
    "#     print(x_test.shape[0], 'test samples')\n",
    "    \n",
    "    curr_epoch = -1\n",
    "    onlyfiles = [f for f in os.listdir('.') if os.path.isfile(os.path.join('.', f)) and f.startswith('moj_ulubiony_model') and f.endswith('.h5')]\n",
    "    if len(onlyfiles) == 0:\n",
    "        print(\"No saved model. Preparing model.\")\n",
    "        imput = Input(shape=(side,side,4))\n",
    "        conv1 = Conv2D(32, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(imput)\n",
    "        conv1 = Conv2D(32,\n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv1)\n",
    "        conv1 = Conv2D(32,\n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv1)\n",
    "        conv1 = Conv2D(32, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv1)\n",
    "#         zero4 = ZeroPadding2D(padding=2)(conv1)\n",
    "        dropout1 = Dropout(0.25)(conv1)\n",
    "        maxpool1 = MaxPooling2D(pool_size=(2, 2))(dropout1)\n",
    "        conv2 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(maxpool1)\n",
    "        conv2 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv2)\n",
    "        conv2 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv2)\n",
    "        conv2 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv2)\n",
    "        dropout2 = Dropout(0.25)(conv2)\n",
    "#         zero = ZeroPadding2D()(dropout2)\n",
    "        maxpool2 = MaxPooling2D(pool_size=(2, 2))(dropout2)\n",
    "        conv3 = Conv2D(128, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(maxpool2)\n",
    "        conv3 = Conv2D(128, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv3)\n",
    "        conv3 = Conv2D(128, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv3)\n",
    "        conv3 = Conv2D(128, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv3)\n",
    "        dropout3 = Dropout(0.25)(conv3)\n",
    "        upsample1 = UpSampling2D(size=(2,2))(dropout3)\n",
    "        concat1 = concatenate([upsample1,dropout2,])#lambda1])\n",
    "        conv4 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(concat1)\n",
    "        conv4 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv4)\n",
    "        conv4 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv4)\n",
    "        conv4 = Conv2D(64, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv4)\n",
    "        dropout4 = Dropout(0.25)(conv4)\n",
    "        upsample2 = UpSampling2D(size=(2,2))(dropout4)\n",
    "#         zpad1 = ZeroPadding2D(((1,0),(1,0)))(upsample2)\n",
    "        concat2 = concatenate([upsample2,conv1])\n",
    "        conv5 = Conv2D(32, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(concat2)\n",
    "        conv5 = Conv2D(32, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv5)\n",
    "        conv5 = Conv2D(1, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv5)\n",
    "        conv5 = Conv2D(1, \n",
    "                       kernel_size=(3,3),\n",
    "                       padding=\"same\", \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l1_l2(0.01),\n",
    "                       bias_regularizer=l1_l2(0.01),\n",
    "                       activity_regularizer=l1_l2(0.01)\n",
    "                      )(conv5)\n",
    "        flat1 = Flatten()(conv5)\n",
    "        danse1 = Dense(280*280)(flat1)\n",
    "        model = Model(inputs=imput, outputs=danse1)\n",
    "\n",
    "        model.compile(loss=create_weighted_binary_crossentropy(0.05,0.95),#keras.losses.binary_crossentropy,#mean_squared_error,#\n",
    "                  optimizer=keras.optimizers.Adam(),       \n",
    "                  metrics=[max_pred,min_pred,'accuracy'])#'accuracy'])#,'precision','recall'])\n",
    "    \n",
    "#     elif len(onlyfiles) == 1:\n",
    "#         print(\"Saved model:\\\"{}\\\"\".format(onlyfiles[0]))\n",
    "#         model = keras.models.load_model(onlyfiles[0])\n",
    "    else:\n",
    "#         onlyfiles = map(lambda y:filter(lambda x:x is not None and x.startswith('epoch'),y.split('.')[0].split('_')),onlyfiles)\n",
    "#         curr_epoch = max(list(map(lambda x:int(x[4:]),onlyfiles)))\n",
    "        curr_epoch = 5\n",
    "        model = keras.models.load_model(\"moj_ulubiony_model_epoch{}.h5\".format(curr_epoch))\n",
    "        print(\"Saved model:\\\"moj_ulubiony_model_epoch{}.h5\\\"\".format(curr_epoch))\n",
    "    curr_epoch += 1\n",
    "    print(\"Current epoch:{}\".format(curr_epoch))\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    for i in range(curr_epoch,epochs):\n",
    "        trainRange = [i for i in range(1,461)]\n",
    "        random.shuffle(trainRange)\n",
    "        for j in range(46):\n",
    "            x_train = np.array([])\n",
    "            y_train = np.array([])\n",
    "            for l in range(10):\n",
    "                x_train = np.append(x_train, unpickleBigDataset2('x', trainRange[j*10+l], trainRange[j*10+l]))\n",
    "                y_train = np.append(y_train, unpickleBigDataset2('y', trainRange[j*10+l], trainRange[j*10+l]))\n",
    "            if input_shape[0]!=4:\n",
    "                x_train = x_train.reshape(200, img_rows, img_cols, 4)\n",
    "                y_train = y_train.reshape(200, img_rows, img_cols, 1)\n",
    "            else:\n",
    "                x_train = x_train.reshape(200, 4, img_rows, img_cols)\n",
    "                y_train = y_train.reshape(200, 1, img_rows, img_cols)\n",
    "            print(x_train.shape)\n",
    "            model.fit(x_train, y_train,\n",
    "                          batch_size=4,\n",
    "                          epochs=1,\n",
    "                          verbose=1,\n",
    "                          validation_data=(x_test, y_test))\n",
    "        model.save(\"moj_ulubiony_model_epoch{}.h5\".format(i))\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Loaded first\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cca3ab272097>\u001b[0m in \u001b[0;36mdoSomeDeepLearning\u001b[1;34m(X, Y, side)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0mHOWMANY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5242c5ae4daa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#         raise MyException()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdoSomeDeepLearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mMyException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#         while ex==1:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-cca3ab272097>\u001b[0m in \u001b[0;36mdoSomeDeepLearning\u001b[1;34m(X, Y, side)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mMyException\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickleBigDataset2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m461\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m499\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickleBigDataset2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m461\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m499\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'channels_first'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f0fe754c1ac3>\u001b[0m in \u001b[0;36munpickleBigDataset2\u001b[1;34m(prefix, start, stop)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loaded first\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTMP_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#     return np.load(os.path.join(TMP_DIR, \"{}.npy\".format(prefix)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\grzegorz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5164\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5165\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5166\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    ex=1\n",
    "    try:\n",
    "#         raise MyException()\n",
    "        doSomeDeepLearning()\n",
    "    except MyException as e:\n",
    "#         while ex==1:\n",
    "#             try:\n",
    "                ex=0\n",
    "                urlX = \"https://www.cs.toronto.edu/~vmnih/data/mass_roads/train/sat/index.html\"\n",
    "                urlY = \"https://www.cs.toronto.edu/~vmnih/data/mass_roads/train/map/index.html\"\n",
    "\n",
    "                if LOAD:\n",
    "                    print(\"Loading images (X)\")\n",
    "#                     X = unpickleBigDataset('f')\n",
    "                if PREPROCESS:\n",
    "                    print(\"Preprocessing images (X)\")\n",
    "#                     X = preprocess(X,preprocessorX,'x')\n",
    "                if LOAD:\n",
    "                    print(\"Loading images (Y)\")\n",
    "                    Y = unpickleBigDataset('z')\n",
    "                if PREPROCESS:\n",
    "                    print(\"Preprocessing images (Y)\")\n",
    "                    Y = preprocess(Y,preprocessorY,'y')\n",
    "\n",
    "                    r = preprocessXY(X,Y)\n",
    "#                     quit()\n",
    "                    print(\"\\n\\t| {}\\t\\t\\t| {}\\t\\t| {}\".format('Hue', 'Saturation', 'Value'))\n",
    "                    l = ['max','min','avg','std','median']\n",
    "                    for i,(c1, c2, c3) in enumerate(r):  \n",
    "                        print(\"{}\\t| {}\\t| {}\\t| {}\".format(l[i],c1, c2, c3))\n",
    "\n",
    "                doSomeDeepLearning(X,Y)\n",
    "#             except Exception:\n",
    "#                 ex=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([[[1,2,3], [1,2,4]],[[4,2,3], [3,2,2]]])\n",
    "np.max(x,axis=2)-np.min(x,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Loaded first\n",
      "Loading...\n",
      "Loaded first\n"
     ]
    }
   ],
   "source": [
    "x_test = unpickleBigDataset2(\"x\",461,499)\n",
    "y_test = unpickleBigDataset2(\"y\",461,499)\n",
    "side=280\n",
    "img_rows, img_cols = side,side\n",
    "if K.image_data_format() == 'channels_first':\n",
    "#             x_train = x_train.reshape(x_train.shape[0], 4, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 4, img_rows, img_cols)\n",
    "#             y_train = y_train.reshape(y_train.shape[0], 1, img_rows, img_cols)\n",
    "    y_test = y_test.reshape(y_test.shape[0], img_rows, img_cols)\n",
    "    input_shape = (4, img_rows, img_cols)\n",
    "else:\n",
    "#             x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 4)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 4)\n",
    "#             y_train = y_train.reshape(y_train.shape[0], img_rows, img_cols, 1)\n",
    "    y_test = y_test.reshape(y_test.shape[0], img_rows, img_cols)\n",
    "    input_shape = (img_rows, img_cols, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 952.2376429006594\n",
      "Test accuracy: 0.9401419809573912\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"moj_ulubiony_model_epoch0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Counter({0.0: 59819200})\n"
     ]
    }
   ],
   "source": [
    "w = model.predict(x_test)\n",
    "# x_test.shape\n",
    "# w=y_test[5:6]\n",
    "print(w.sum())\n",
    "print(collections.Counter(w.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9c907d93ec77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mx_test\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m/=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m110\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "def to3ch(im):\n",
    "    side = 750\n",
    "    im = im.reshape(side, side, 1)\n",
    "    img = np.concatenate((im,im,im),axis=2)\n",
    "    print(img.shape)\n",
    "#     print(img)\n",
    "    return img\n",
    "\n",
    "x_test +=1\n",
    "x_test /=2\n",
    "for i in range(100,110):\n",
    "    img = to3ch(y_test[i])\n",
    "    matplotlib.image.imsave(\"a\"+str(i),img)\n",
    "    matplotlib.image.imsave(\"b\"+str(i),x_test[i][...,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Loaded first\n",
      "Loading...\n",
      "Loaded first\n",
      "(750, 750, 3)\n"
     ]
    }
   ],
   "source": [
    "x = unpickleBigDataset2('f',5,8)\n",
    "y = unpickleBigDataset2('z',5,8)\n",
    "x= x.reshape(x.shape[0], 750,750,3)\n",
    "y=y.reshape(y.shape[0], 750,750)\n",
    "img = to3ch(y[0])\n",
    "matplotlib.image.imsave(\"a\"+str(12),img)\n",
    "matplotlib.image.imsave(\"b\"+str(12),x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
