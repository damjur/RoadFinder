{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D,UpSampling2D,Lambda\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOWMANY = 400\n",
    "DEBUG = True\n",
    "TMP_DIR = 'tmp'\n",
    "FORCE_RELOAD = False#True\n",
    "LOAD = True\n",
    "PREPROCESS = True#False\n",
    "batch_size = 128   # ile obrazków przetwarzamy na raz (aktualizacja wag sieci następuje raz na całą grupę obrazków)\n",
    "epochs = 12         # ile epok będziemy uczyli\n",
    "SIZE = (750,750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(url):\n",
    "    raw = urllib.request.urlopen(url).read()\n",
    "    npraw= np.array(bytearray(raw),dtype=np.uint8)\n",
    "    return cv2.imdecode(npraw,-1)#-1 -> as is (with the alpha channel)\n",
    "\n",
    "def getImageName(url):\n",
    "    return url.split('/').pop().split('.').pop(0)\n",
    "\n",
    "def pickleBigDataset(prefix,dataset,size):\n",
    "#     onlyfiles = [f for f in os.listdir(TMP_DIR) if os.path.isfile(os.path.join(TMP_DIR, f)) and f[0]==prefix]\n",
    "#     j = int(np.ceil(len(dataset)/size))\n",
    "#     for i in tqdm_notebook(range(1,j+1)):\n",
    "#         with open(os.path.join(TMP_DIR, prefix+str(i)),'wb') as f:\n",
    "#             pickle.dump(dataset[size*(i-1):size*i],f)\n",
    "    np.save(os.path.join(TMP_DIR, prefix),dataset)\n",
    "\n",
    "def unpickleBigDataset(prefix):\n",
    "#     onlyfiles = [f for f in os.listdir(TMP_DIR) if os.path.isfile(os.path.join(TMP_DIR, f)) and f[0]==prefix]\n",
    "#     dataset = []\n",
    "#     if len(onlyfiles)>0:\n",
    "#         for f in tqdm_notebook(onlyfiles):\n",
    "#             with open(os.path.join(TMP_DIR, f),'rb') as fl:\n",
    "#                 dataset+=pickle.load(fl)\n",
    "#     return dataset\n",
    "    return np.load(os.path.join(TMP_DIR, \"{}.npy\".format(prefix)))\n",
    "    \n",
    "            \n",
    "def loadImagesFromSite(url,prefix):\n",
    "    onlyfiles = [f for f in os.listdir(TMP_DIR) if os.path.isfile(os.path.join(TMP_DIR, f)) and f.startswith(prefix)]\n",
    "    if len(onlyfiles)==0 or FORCE_RELOAD:\n",
    "        imgs = []\n",
    "        I = None\n",
    "        \n",
    "    else:\n",
    "        imgs = unpickleBigDataset(prefix)\n",
    "        I = len(imgs)\n",
    "    if len(imgs)!=1109:\n",
    "        print(\"Loading images from {}\".format(url))\n",
    "        print(\"Cached images {}. Proceeding from {} image.\".format(len(imgs),I if I is not None else 0))\n",
    "\n",
    "        s = 100\n",
    "\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html = BeautifulSoup(response.read(),\"lxml\")\n",
    "            i = I if I is not None else 0\n",
    "            links = html.find_all('a')[I:HOWMANY]\n",
    "            for link in tqdm_notebook(links):\n",
    "                img = loadImage(link.get('href'))    \n",
    "                imgs += [cv2.resize(img,SIZE)]\n",
    "                if i%s==0:\n",
    "                    pickleBigDataset(prefix,imgs,s)\n",
    "                i+=1\n",
    "        pickleBigDataset(prefix,imgs,s)\n",
    "    \n",
    "        \n",
    "    return np.array(imgs)  \n",
    "\n",
    "def saveDataset(X,Y,prefix=\"\"):\n",
    "    with open('pickledDatasetX'+prefix,'wb') as f:\n",
    "        pickle.dump(X,f)\n",
    "    with open('pickledDatasetY'+prefix,'wb') as f:\n",
    "        pickle.dump(Y,f)\n",
    "        \n",
    "def loadDataset(prefix=\"\"):\n",
    "    try:\n",
    "        X = unpickleBigDataset('x')\n",
    "        Y = unpickleBigDataset('y')\n",
    "        return X,Y\n",
    "    except:\n",
    "        print(\"Failed loading dataset from file system\")\n",
    "        return None,None\n",
    "    \n",
    "def display(X,Y,howmany=None):\n",
    "    if howmany is None:\n",
    "        howmany = X.shape[0]\n",
    "        \n",
    "    for i in range(howmany):\n",
    "        print(X[i].max(),X[i].min())\n",
    "        plt.figure()\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(X[i])\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(Y[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_patches(image,size,side,imposition):\n",
    "#     patches = []\n",
    "    \n",
    "    \n",
    "#     for i in range(int(size[0]/side)):\n",
    "#         for j in range(int(size[1]/side)):\n",
    "#             patches += [image[i*side:(i+1)*side,j*side:(j+1)*side]]\n",
    "#     return patches\n",
    "\n",
    "def get_patches(image,size,side,imposition):\n",
    "    patches = []\n",
    "    \n",
    "    img = np.zeros((image.shape[0]+imposition,image.shape[1]+imposition,3))\n",
    "    for i in range(3):\n",
    "        img[...,i] = np.pad(image[...,i],((imposition,0),(imposition,0)),'reflect')\n",
    "    image = img\n",
    "    for i in range(int(size[0]/side)):\n",
    "        for j in range(int(size[1]/side)):\n",
    "            imp1=np.max([i*side-imposition,0])\n",
    "            imp2=(i+1)*side+imposition if imp1!=0 else (i+1)*side+imposition*2\n",
    "            imp3=np.max([j*side-imposition,0])\n",
    "            imp4=(j+1)*side+imposition if imp3!=0 else (j+1)*side+imposition*2\n",
    "            patches += [image[imp1:imp2,imp3:imp4]]\n",
    "    return patches\n",
    "\n",
    "def resize(image):\n",
    "#     try:\n",
    "    size = (750,750)\n",
    "    side = 78\n",
    "    imposition = 5\n",
    "    if size!=SIZE:\n",
    "        return cv2.resize(image,size),size,side,imposition\n",
    "    else:\n",
    "        return image,size,side,imposition\n",
    "#     except:\n",
    "#         print('Failed resizing... Image shall be ignored')\n",
    "#         return None,None,None#I tak jest ich mnóstwo\n",
    "\n",
    "def preprocessorX(image):\n",
    "    image,size,side,imposition = resize(image)\n",
    "    if image is not None:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        if image.max() > 1:\n",
    "            image /= 255\n",
    "\n",
    "        for i in range(3):\n",
    "            image[...,i] -= image[...,i].mean()\n",
    "            image[...,i] /= image[...,i].std()\n",
    "\n",
    "        #remove outliers\n",
    "        image[image<-3] = -3\n",
    "        image[image>3] = 3\n",
    "        \n",
    "        #between -1,1\n",
    "        for i in range(3):\n",
    "            image[...,i] /= np.max(np.abs([image[...,i].min(),image[...,i].max()]))\n",
    "\n",
    "        return get_patches(image,size,side,imposition),False\n",
    "    return [],True\n",
    "    \n",
    "\n",
    "def preprocessorY(image):\n",
    "    image,size,side,imposition = resize(image)    \n",
    "    if image is not None:\n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        if image.max() > 1:\n",
    "            image /= 255\n",
    "        \n",
    "        for i in range(3):\n",
    "            image[...,i] = (image[...,i] - image[...,i].min())/(image[...,i].max() - image[...,i].min())\n",
    "\n",
    "        return get_patches(image,size,side,imposition),False\n",
    "    return [],True\n",
    "\n",
    "def getRoadStats(arr,mask):\n",
    "    b = mask.astype(np.bool)\n",
    "    x = arr[b]\n",
    "    if len(x) != 0:\n",
    "        return [x.max(0),x.min(0),x.mean(0),x.std(0),np.median(x,axis=0)]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def preprocessXY(X,Y,i1,i2):\n",
    "    print(i1,i2)\n",
    "    #TODO remove those not ignored in one, but ignored in other\n",
    "    \n",
    "    r = []\n",
    "    for i in range(len(X)):\n",
    "        s = getRoadStats(X[i],Y[i])\n",
    "        if s is not None:\n",
    "            r += [s]\n",
    "    return np.array(r).mean(0)\n",
    "    \n",
    "\n",
    "def preprocess(images,preprocessor,prefix):\n",
    "    onlyfiles = [f for f in os.listdir(TMP_DIR) if os.path.isfile(os.path.join(TMP_DIR, f)) and f.startswith(prefix)]\n",
    "    if len(onlyfiles)==0:\n",
    "        result = []\n",
    "        I = None\n",
    "    else:\n",
    "        imgs = unpickleBigDataset(prefix)\n",
    "        I = len(imgs)\n",
    "    print(\"Cached images {}. Proceeding from {} image.\".format(len(result),I if I is not None else 0))\n",
    "    \n",
    "    result = []\n",
    "    s = 400\n",
    "    print(\"Preprocessing images.\")\n",
    "    i = I if I is not None else 0\n",
    "    ignoring = []\n",
    "    for image in tqdm_notebook(images[I:]):\n",
    "        r,ignored = preprocessor(image)\n",
    "        \n",
    "        result += r\n",
    "        \n",
    "        if i%s==0:\n",
    "            pickleBigDataset(prefix,result,s)\n",
    "    images = None\n",
    "        \n",
    "    return np.array(result),np.array(ignoring)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doSomeDeepLearning(X,Y,side=85):\n",
    "    num_classes = 2    # ile klas będziemy rozpoznawali\n",
    "\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = side,side   # takie wymiary mają obrazki w bazie MNIST\n",
    "\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    x_train = unpickleBigDataset('xain')\n",
    "    y_train = unpickleBigDataset('yain')\n",
    "    x_test = unpickleBigDataset('xest')\n",
    "    y_test = unpickleBigDataset('yest')\n",
    "    if len(x_train) = 0 or len(x_test) = 0 or len(y_train)=0 or len(y_test)=0:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "    # TensorFlow i Theano domyślnie inaczej interpretują kolejne wymiary tensora\n",
    "    # TensorFlow: [batch, height, width, channels]\n",
    "    #     Theano: [batch, channels, height, width]\n",
    "    # \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    \n",
    "    curr_epoch = -1\n",
    "    onlyfiles = [f for f in os.listdir('.') if os.path.isfile(os.path.join('.', f)) and f.startswith('moj_ulubiony_model') and f.endswith('.h5')]\n",
    "    if len(onlyfiles) == 0:\n",
    "        print(\"No saved model. Preparing model.\")\n",
    "        imput = Input(shape=(side,side,3))\n",
    "        conv1 = Conv2D(32, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(imput)\n",
    "        conv1 = Conv2D(32, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(conv1)\n",
    "        conv1 = Conv2D(32, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(conv1)\n",
    "        dropout1 = Dropout(0.25)(conv1)\n",
    "        maxpool1 = MaxPooling2D(pool_size=(2, 2))(dropout1)\n",
    "        conv2 = Conv2D(64, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(maxpool1)\n",
    "        conv2 = Conv2D(64, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(conv2)\n",
    "        conv2 = Conv2D(64, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(conv2)\n",
    "        dropout2 = Dropout(0.25)(conv2)\n",
    "        maxpool2 = MaxPooling2D(pool_size=(2, 2))(dropout2)\n",
    "        conv3 = Conv2D(128, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(maxpool2)\n",
    "        conv3 = Conv2D(128, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(conv3)\n",
    "        conv3 = Conv2D(128, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(conv3)\n",
    "        dropout3 = Dropout(0.25)(conv3)\n",
    "        upsample1 = UpSampling2D(size=(2,2))(dropout3)\n",
    "        concat1 = concatenate([upsample1,conv2])\n",
    "        conv4 = Conv2D(64, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(concat1)\n",
    "        conv4 = Conv2D(64, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(conv4)\n",
    "        conv4 = Conv2D(64, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(conv4)\n",
    "        dropout4 = Dropout(0.25)(conv4)\n",
    "        upsample2 = UpSampling2D(size=(2,2))(dropout4)\n",
    "        concat2 = concatenate([upsample2,conv1])\n",
    "        conv5 = Conv2D(32, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(concat2)\n",
    "        conv5 = Conv2D(32, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(conv5)\n",
    "        conv5 = Conv2D(1, kernel_size=(3,3), activation='relu',padding=\"same\",kernel_initializer='he_normal')(conv5)\n",
    "        model = Model(inputs=imput, outputs=conv5)\n",
    "\n",
    "        model.compile(loss=keras.losses.mean_squared_error,\n",
    "                  optimizer=keras.optimizers.Adam(),       \n",
    "                  metrics=['accuracy']) \n",
    "    elif len(onlyfiles) == 1:\n",
    "        print(\"Saved model:\\\"{}\\\"\".format(onlyfiles[0]))\n",
    "        model = keras.models.load_model(onlyfiles[0])\n",
    "    else:\n",
    "        onlyfiles = map(lambda y:filter(lambda x:x is not None and x.startswith('epoch'),y.split('.')[0].split('_')),onlyfiles)\n",
    "        curr_epoch = max(list(map(lambda x:int(x[4:]),onlyfiles)))\n",
    "        model = keras.models.load_model(\"moj_ulubiony_model_epoch{}.h5\".format(curr_epoch))\n",
    "        print(\"Saved model:\\\"moj_ulubiony_model_epoch{}.h5\\\"\".format(curr_epoch))\n",
    "    curr_epoch += 1\n",
    "    print(\"Current epoch:{}\".format(curr_epoch))\n",
    "    model.summary()\n",
    "    for layer in model.layers:\n",
    "        print(layer.get_config())\n",
    "    \n",
    "    \n",
    "    for i in range(curr_epoch,epochs):\n",
    "        model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=1,\n",
    "                      verbose=1,\n",
    "                      validation_data=(x_test, y_test))\n",
    "        model.save(\"moj_ulubiony_model_epoch{}.h5\".format(i))\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed loading dataset from file system\n",
      "Loading images\n",
      "z []\n",
      "A1\n",
      "Loading images from https://www.cs.toronto.edu/~vmnih/data/mass_roads/train/map/index.html\n",
      "Cached images 0. Proceeding from 0 image.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad64da6f9ee4cba8919446b3030ce42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    urlX = \"https://www.cs.toronto.edu/~vmnih/data/mass_roads/train/sat/index.html\"\n",
    "    urlY = \"https://www.cs.toronto.edu/~vmnih/data/mass_roads/train/map/index.html\"\n",
    "    \n",
    "    X,Y = loadDataset()\n",
    "    if X is None and Y is None or DEBUG:\n",
    "        if LOAD:\n",
    "            print(\"Loading images\")\n",
    "            Y = loadImagesFromSite(urlY,'z')\n",
    "            X = loadImagesFromSite(urlX,'f')\n",
    "        if PREPROCESS:\n",
    "            print(\"Preprocessing images\")\n",
    "            X,i1 = preprocess(X,preprocessorX,'x')\n",
    "            Y,i2 = preprocess(Y,preprocessorY,'y')\n",
    "        \n",
    "            print(preprocessXY(X,Y,i1,i2))\n",
    "        \n",
    "    doSomeDeepLearning(X,Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
